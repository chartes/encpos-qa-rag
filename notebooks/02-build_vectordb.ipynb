{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02 - Building the vector database",
   "id": "30600691bf5c8d28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notebook steps:\n",
    "\n",
    "- Load TSV that contains the chunks to index\n",
    "- Generate vector indexing configurations from `config.yml`\n",
    "- Index the chunks in the vector database using the `VectorDB` abstraction\n",
    "\n",
    "\n",
    "When the chunks are encoded by the embedding model, they are stored in a vector database. When a user enters a query, it is also encoded by the same model, and then compared to the vectors in the database to identify the most similar documents.\n",
    "\n",
    "The main technical challenge is as follows:\n",
    "> Given a query vector, quickly find its **k nearest neighbors** in the vector database, i.e., the k most relevant documents.\n",
    "\n",
    "\n",
    "Let's start by create the vector database and save it to disk.\n",
    "\n"
   ],
   "id": "7258a0e8f82e5852"
  },
  {
   "cell_type": "code",
   "id": "4d7515da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:19:06.235477Z",
     "start_time": "2025-07-17T10:19:01.036318Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "from lib.io_utils import read_yaml, get_absolute_path\n",
    "from lib.vector_store import VectorDB\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9f2c7612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:19:06.256716Z",
     "start_time": "2025-07-17T10:19:06.249943Z"
    }
   },
   "source": [
    "## configuration loading\n",
    "config = read_yaml(\"../config.yml\")\n",
    "data_path = get_absolute_path(\"data/raw/encpos_chunked_tok_512_51.csv\")\n",
    "defaults = config.get(\"defaults\", {})"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "44bceb3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:19:07.411416Z",
     "start_time": "2025-07-17T10:19:06.366228Z"
    }
   },
   "source": [
    "## data loading\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"File Not Found: {data_path}. Run first the notebook: 01-prepare_chunk_corpus.ipynb.\")\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "print(f\"Total chunks to index: {len(df)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to index: 39377\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We prepare the configurations to create our persistent vector databases.",
   "id": "ad2cc6d69892a920"
  },
  {
   "cell_type": "code",
   "id": "1a922b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:19:07.425173Z",
     "start_time": "2025-07-17T10:19:07.421804Z"
    }
   },
   "source": [
    "vector_indexing = []\n",
    "for entry in config.get(\"vector_indexing\", []):\n",
    "    model_id = entry[\"model_id\"]\n",
    "    model = next((m for m in config[\"embedding_models\"] if m[\"id\"] == model_id), None)\n",
    "    if not model:\n",
    "        raise ValueError(f\"Mod√®le non trouv√© : {model_id}\")\n",
    "\n",
    "    for backend in entry[\"backends\"]:\n",
    "        suffix = f\"{model_id}_{backend}\"\n",
    "        name = f\"{model['name']} - {backend.upper()}\"\n",
    "        collection_name = f\"{defaults.get('collection_prefix', 'encpos')}_{model_id}\"\n",
    "        path = os.path.join(defaults.get(\"base_path\", \"data/vectordb\"), suffix)\n",
    "\n",
    "        vector_indexing.append({\n",
    "            \"name\": name,\n",
    "            \"backend\": backend,\n",
    "            \"embedding_model\": model[\"model_path\"],\n",
    "            \"metric\": defaults.get(\"metric\", \"cosine\"),\n",
    "            \"text_column\": defaults.get(\"text_column\", \"full_chunk\"),\n",
    "            \"metadata_columns\": defaults.get(\"metadata_columns\", []),\n",
    "            \"path\": path,\n",
    "            \"qdrant_collection_name\": collection_name,\n",
    "            \"k\": defaults.get(\"k\", 10),\n",
    "            \"force_rebuild\": defaults.get(\"force_rebuild\", False)\n",
    "        })"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To efficiently index our data in a vector database that could have thousands of documents, we need to pick two key things:\n",
    "\n",
    "- A **distance metric** to compare vectors (like cosine similarity, Euclidean distance, etc.);\n",
    "\n",
    "- A **nearest neighbors search algorithm** to quickly find relevant documents.\n",
    "\n",
    "In the following cell, we have set up a main loop that builds a vector database for each configuration defined in the `config.yml` file.\n",
    "The goal is to test different combinations of embedding models and storage backends.\n",
    "\n",
    "The two backends currently supported are:\n",
    "\n",
    "- Faiss: very fast for indexing and searching, but metadata filters are limited;\n",
    "\n",
    "- LanceDB: slower for indexing, but allows complex queries on metadata, for example in SQL.\n",
    "\n",
    "We chose to use cosine similarity as the metric for comparing vectors. It measures the angle between two vectors, which allows their direction to be compared independently of their norm. This requires normalizing all vectors (i.e., giving them a unit norm) before indexing or searching.\n",
    "\n",
    "To facilitate indexing, we have developed a Python abstraction called `VectorDB` that supports:\n",
    "\n",
    "- Vector normalization\n",
    "\n",
    "- Creation of vector databases and their persistence\n",
    "\n",
    "- Indexing of embeddings\n",
    "\n",
    "- Searching\n",
    "\n",
    "This abstraction allows us to compare different models and backends in a uniform manner and evaluate them under fair conditions.\n",
    "\n"
   ],
   "id": "1b548879d9b99db8"
  },
  {
   "cell_type": "code",
   "id": "85445417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:21:02.830769Z",
     "start_time": "2025-07-17T10:20:04.744963Z"
    }
   },
   "source": [
    "%%time\n",
    "#df = df.sample(n=50)\n",
    "for conf in vector_indexing:\n",
    "    print(\"\\n--- Indexation in progress ---\")\n",
    "    print(\"Nom:\", conf[\"name\"])\n",
    "\n",
    "    db = VectorDB(\n",
    "        backend=conf[\"backend\"],\n",
    "        embedding_model=conf[\"embedding_model\"],\n",
    "        metric=conf[\"metric\"],\n",
    "        path=get_absolute_path(conf[\"path\"]),\n",
    "        k=conf[\"k\"],\n",
    "        force_rebuild=bool(conf[\"force_rebuild\"])\n",
    "    )\n",
    "\n",
    "\n",
    "    db.add_from_dataframe(\n",
    "        df=df,\n",
    "        text_column=conf[\"text_column\"],\n",
    "        metadata_columns=conf[\"metadata_columns\"]\n",
    "    )\n",
    "\n",
    "    db.save()\n",
    "    print(\"üì¶ Index is created:\", conf[\"name\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: CamemBERT Large - FAISS\n",
      "üìÇ Loading existing FAISS index from /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/camembert-large_faiss\n",
      "‚ÑπÔ∏è FAISS index already loaded ‚Äî skipping creation.\n",
      "üì¶ Index is created: CamemBERT Large - FAISS\n",
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: CamemBERT Large - LANCEDB\n",
      "üì¶ LanceDB intialization on: /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/camembert-large_lancedb\n",
      "‚ÑπÔ∏è LanceDB table founded.\n",
      "üì¶ Index is created: CamemBERT Large - LANCEDB\n",
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: CamemBERT Base - FAISS\n",
      "üìÇ Loading existing FAISS index from /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/camembert-base_faiss\n",
      "‚ÑπÔ∏è FAISS index already loaded ‚Äî skipping creation.\n",
      "üì¶ Index is created: CamemBERT Base - FAISS\n",
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: CamemBERT Base - LANCEDB\n",
      "üì¶ LanceDB intialization on: /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/camembert-base_lancedb\n",
      "‚ÑπÔ∏è LanceDB table founded.\n",
      "üì¶ Index is created: CamemBERT Base - LANCEDB\n",
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: Multilingual DistilUSE - FAISS\n",
      "üìÇ Loading existing FAISS index from /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/multilingual_faiss\n",
      "‚ÑπÔ∏è FAISS index already loaded ‚Äî skipping creation.\n",
      "üì¶ Index is created: Multilingual DistilUSE - FAISS\n",
      "\n",
      "--- Indexation in progress ---\n",
      "Nom: Multilingual DistilUSE - LANCEDB\n",
      "üì¶ LanceDB intialization on: /Users/lucaterre/Documents/pro/Travail_courant/DEV/AI-ENC-Projects/on-github/encpos-qa-rag/data/retrievers/vectordb/multilingual_lancedb\n",
      "‚ÑπÔ∏è LanceDB table founded.\n",
      "üì¶ Index is created: Multilingual DistilUSE - LANCEDB\n",
      "CPU times: user 2.57 s, sys: 11.9 s, total: 14.5 s\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dee931056f69ba19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "‚û°Ô∏è Notebook suivant : [03-assemble_rag.ipynb](./03-assemble_rag.ipynb)",
   "id": "6812a98481fd9b54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T10:19:23.359491Z",
     "start_time": "2025-07-17T10:19:23.357758Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ef67d4eced927718",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
