{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8aa6ddc3f16fc9",
   "metadata": {},
   "source": [
    "# 01 - Prepare & chunk corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95098c4f20814e72",
   "metadata": {},
   "source": [
    "Notebook steps:\n",
    "- Load TSV file that contains the longform abstracts scrapped (\"positions de thèses\")\n",
    "- Data analysis and cleaning\n",
    "- Apply custom chunking strategy and save the calculated chunks to a new TSV file for later indexing\n",
    "\n",
    "ℹ️ All the parameters (models, paths, chunking, etc.) are externalized in [`config.yml`](../config.yml), to easily re-run the notebook with different settings."
   ]
  },
  {
   "cell_type": "code",
   "id": "e550c4a13d237286",
   "metadata": {},
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from utils.io_utils import (get_absolute_path,\n",
    "                            read_yaml)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a6add4b690cd5bd",
   "metadata": {},
   "source": "## Configuration loading"
  },
  {
   "cell_type": "code",
   "id": "25980a1c",
   "metadata": {},
   "source": [
    "config = read_yaml(\"../config.yml\")\n",
    "\n",
    "data_path = get_absolute_path(config['data']['source'])\n",
    "TEXT_COL = config['data']['text_column']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d46c449a165d815",
   "metadata": {},
   "source": [
    "##  Data loading\n",
    "\n",
    "The dataset contains 3057 longform abstracts ([https://theses.chartes.psl.eu/](https://theses.chartes.psl.eu/)) from the PSL University, scraped from the [DoTS API](https://api.dots.psl.eu/). Each thesis abstract is divided into sections (e.g. introduction, chapters, conclusion, appendices, etc.) to allow for semantic chunking of the document.\n",
    "\n",
    "Data model\n",
    "\n",
    "---\n",
    "\n",
    "- `text` : the text of the document that will be vectorized indexed. Each text row corresponding to a signifiant section of the document (e.g. introduction, chapters, conclusion, appendices, etc.) and corresponding to a first \"semantic chunking\" of the document.\n",
    "\n",
    "and the metadata columns (important to retrieve and identify the document later):\n",
    "- `unique_id` : the unique identifier of the document (e.g. \"ENCPOS_2023_01_5\"; \"01\": first thesis of the collection year 2023, \"5\": section 5 of the thesis)\n",
    "- `file_id` : the identifier of the document (util for research in Web)\n",
    "- `author` : the author of the document (positions de thèses)\n",
    "- `section` : the section title of the document (e.g. introduction, chapters, conclusion, appendices, etc.)\n",
    "- `title` : the title of the document\n",
    "- `position_name` : the name of the position\n",
    "- `year` : the year of the document\n",
    "- `text_token_length` : the length of the text in tokens"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc39eacd",
   "metadata": {},
   "source": [
    "df = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "# First ensure the columns are in the right format: Check if the columns exist and convert them to the appropriate types, check non null values\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)\n",
    "for col in ['file_id', 'unique_id', 'author', 'section', 'text', 'position_name']:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5807d5325634c41",
   "metadata": {},
   "source": "## Minimal dataset analysis"
  },
  {
   "cell_type": "code",
   "id": "a90b4a3d",
   "metadata": {},
   "source": [
    "print(\"Total number of sections:\", len(df))\n",
    "max_text_token_length = df['text_token_length'].max()\n",
    "min_text_token_length = df['text_token_length'].min()\n",
    "mean_text_token_length = df['text_token_length'].mean()\n",
    "print(\"Max token length:\", int(max_text_token_length))\n",
    "print(\"Min token length:\", int(min_text_token_length))\n",
    "print(\"Mean token length:\", int(mean_text_token_length))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c70e33e61faee709",
   "metadata": {},
   "source": [
    "\n",
    "We observe that most sections have a reasonable length, but there are also very short sections (< 3 tokens). The question is whether to keep them or not. Let's look at some examples of very short sections."
   ]
  },
  {
   "cell_type": "code",
   "id": "b3e9be4b34bf144c",
   "metadata": {},
   "source": [
    "examples = df[df['text_token_length'] < 6].drop_duplicates().head(10)\n",
    "print(\"\\nShort sections sample:\")\n",
    "for i, row in examples.iterrows():\n",
    "    print(f\"{i+1}. '{row[TEXT_COL]}' (tokens: {row['text_token_length']})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39c84b2b9efc98a4",
   "metadata": {},
   "source": [
    "\n",
    "- The section < 4 tokens are likely to be noise and should be removed.\n",
    "- The section > 4 tokens are likely to be useful (like named entities or important concepts) and should be kept."
   ]
  },
  {
   "cell_type": "code",
   "id": "c38c7249",
   "metadata": {},
   "source": [
    "df = df[df['text_token_length'] >= 4].reset_index(drop=True)\n",
    "print(\"\\nTotal sections after filtering short sections (>4 tokens):\", len(df))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85943729",
   "metadata": {},
   "source": [
    "## Histogramme de la longueur des textes\n",
    "def plot_histogram(dataframe, column, title, xlabel, ylabel, bins=50):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    dataframe[column].hist(bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_histogram(\n",
    "    df,\n",
    "    'text_token_length',\n",
    "    'Token length distribution',\n",
    "    'Tokens',\n",
    "    'Sections',\n",
    "     bins=50\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93fd6744adc4ac29",
   "metadata": {},
   "source": [
    "\n",
    "Another issue with dataset is that some sections are very long (more than 1000 tokens).\n",
    "\n",
    "These sections may exceed the input limit of some embedding models we want to apply later.\n",
    "\n",
    "Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "id": "b1af309009be7b9d",
   "metadata": {},
   "source": [
    "camembert_base_max_length = SentenceTransformer('Lajavaness/sentence-camembert-base').max_seq_length\n",
    "camembert_large_max_length = SentenceTransformer('Lajavaness/sentence-camembert-large').max_seq_length\n",
    "multilingual_max_length = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1').max_seq_length\n",
    "\n",
    "\n",
    "print(f\"CamemBERT base max length: {camembert_base_max_length}\")\n",
    "print(f\"CamemBERT large max length: {camembert_large_max_length}\")\n",
    "print(f\"Multilingual base max length: {multilingual_max_length}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8496d016e73e409",
   "metadata": {},
   "source": "To prevent this limitation, we will apply a custom chunking strategy that will split the text into smaller chunks while preserving the context of each section."
  },
  {
   "cell_type": "markdown",
   "id": "5a3472cafb8494c0",
   "metadata": {},
   "source": [
    "## Chunking method\n",
    "\n",
    "It exists many ways to split a text into chunks (characters, sentences, paragraphs, etc.): There is no universal method, but you have to choose the method that is most suitable for your use case.\n",
    "\n",
    "Two parameters are important to consider:\n",
    "- `CHUNK_SIZE`: the maximum size of the chunks in tokens (e.g. 512 tokens for CamemBERT) ;\n",
    "- `CHUNK_OVERLAP`: the number of tokens that overlap between chunks (e.g. 10% of the chunk size, i.e. 51 tokens for a chunk of 512 tokens).\n",
    "\n",
    "Ici on utilise une stratégie de double chunking :\n",
    "\n",
    "- Le premier découpage est déjà effectué via le scraping par section (découpage sémantique) Cf.supra.\n",
    "- Le second est un découpage par tokens, car les modèles d'embedding ont une limite fixe d'entrée (ex: 514 tokens).\n",
    "- Chaque chunk est enrichi avec le titre de la section pour donner du contexte (ex: \"Introduction : le ...\").\n",
    "\n",
    "> we using RecursiveCharacterTextSplitter from langchain to split the text into chunks, this not chunking by characters but by tokens because we explicitly use \"from_huggingface_tokenizer\" method (Check doc: https://python.langchain.com/docs/how_to/split_by_token/#hugging-face-tokenizer)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CHUNK_SIZE = config['chunking']['chunk_size']\n",
    "\n",
    "# Compute the chunk overlap as 10% of the chunk size. This allows to dynamically define the chunk overlap to avoid size errors.\n",
    "CHUNK_OVERLAP =  int(CHUNK_SIZE / 10)"
   ],
   "id": "2246de4f63bd2b33",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb6c5189",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Lajavaness/sentence-camembert-large\", use_fast=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b98435e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in the text, including special tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    return len(tokenizer(text, add_special_tokens=True)['input_ids'])\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by: removing extra spaces, normalizing unicode characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", \" \".join(text.strip().split()))\n",
    "\n",
    "def remove_special_tokens_ids(token_ids: list[int]) -> list[int]:\n",
    "    \"\"\"Delete special tokens from a list of token IDs.\n",
    "\n",
    "    Args:\n",
    "        token_ids (list[int]): List of token IDs to clean.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: List of token IDs without special tokens.\n",
    "    \"\"\"\n",
    "    special_ids = tokenizer.all_special_ids\n",
    "    return [tok for tok in token_ids if tok not in special_ids]\n",
    "\n",
    "def process_row_recursive(row: pd.Series) -> list[dict]:\n",
    "    \"\"\"Apply the chunking at token level on a DataFrame row:\n",
    "    - Cleans the text and adds the title as a prefix.\n",
    "    - Splits the text into fixed-size chunks (special tokens included).\n",
    "    - Saves the chunk WITHOUT special tokens (for human use / display).\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row of the DataFrame containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the chunk information.\n",
    "    \"\"\"\n",
    "    text = clean_text(row[TEXT_COL])\n",
    "    title = f\"{clean_text(row['position_name'])} : \"\n",
    "    len_title = count_tokens(title)\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    results = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_token_ids = tokenizer(chunk, add_special_tokens=True)[\"input_ids\"]\n",
    "        total_tokens = len_title + len(chunk_token_ids)\n",
    "\n",
    "        # Truncating the chunk if it exceeds the maximum allowed size\n",
    "        if total_tokens > CHUNK_SIZE:\n",
    "            allowed = CHUNK_SIZE - len_title\n",
    "            chunk_token_ids = chunk_token_ids[:allowed]\n",
    "\n",
    "\n",
    "        # Chunk clean: remove special tokens for saving\n",
    "        chunk_token_ids_clean = remove_special_tokens_ids(chunk_token_ids)\n",
    "        chunk_text = tokenizer.decode(chunk_token_ids_clean, skip_special_tokens=True)\n",
    "\n",
    "        # Texte complet avec titre + chunk nettoyé\n",
    "        full_chunk = f\"{title}{chunk_text}\".strip()\n",
    "\n",
    "        results.append({\n",
    "            \"unique_id\": row[\"unique_id\"],\n",
    "            \"chunk_id\": f\"{row['unique_id']}_chunk_{i}\",\n",
    "            \"file_id\": row.get(\"file_id\"),\n",
    "            \"author\": row.get(\"author\"),\n",
    "            \"position_name\": clean_text(row.get(\"position_name\")),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"section\": row.get(\"section\"),\n",
    "            \"raw_chunk\": chunk_text,\n",
    "            \"full_chunk\": full_chunk,\n",
    "            \"full_chunk_characters_len\": len(full_chunk),\n",
    "            \"full_chunk_token_len\": count_tokens(full_chunk),  # avec tokens spéciaux inclus\n",
    "        })\n",
    "\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a38011c",
   "metadata": {},
   "source": [
    "# Apply chunking to the DataFrame and save the results.\n",
    "out_path = f\"../../data/raw/encpos_chunked_tok_{CHUNK_SIZE}_{CHUNK_OVERLAP}-test.csv\"\n",
    "if os.path.exists(out_path):\n",
    "    print(f\"⚠️ Fichier déjà généré : {out_path}. Supprimez-le pour forcer le recalcul.\")\n",
    "    df_chunks = pd.read_csv(out_path, sep=\"\\t\")\n",
    "else:\n",
    "    # chunking accelerated by parallel processing\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_row_recursive)(row) for _, row in tqdm(df.iterrows(), total=len(df))\n",
    "    )\n",
    "    flat_chunks = [item for sublist in results for item in sublist]\n",
    "    # chunks dedupe (just in case)\n",
    "    df_chunks = pd.DataFrame(flat_chunks).drop_duplicates(subset=[\"full_chunk\", \"unique_id\"]).reset_index(drop=True)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df_chunks.to_csv(out_path, sep=\"\\t\", encoding=\"utf-8\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "70c183097972df72",
   "metadata": {},
   "source": "## Post-chunking analysis"
  },
  {
   "cell_type": "code",
   "id": "4f271a0b",
   "metadata": {},
   "source": [
    "## Statistiques des chunks\n",
    "print(\"Total chunks generated:\", len(df_chunks))\n",
    "max_chunk_token_length = df_chunks['full_chunk_token_len'].max()\n",
    "min_chunk_token_length = df_chunks['full_chunk_token_len'].min()\n",
    "mean_chunk_token_length = df_chunks['full_chunk_token_len'].mean()\n",
    "print(\"Max token length:\", int(max_chunk_token_length))\n",
    "print(\"Min token length:\", int(min_chunk_token_length))\n",
    "print(\"Mean token length:\", int(mean_chunk_token_length))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f3cf44e",
   "metadata": {},
   "source": [
    "## Histogramme après découpe\n",
    "plot_histogram(\n",
    "    df_chunks,\n",
    "    'full_chunk_token_len',\n",
    "    'Token length distribution (after chunking)',\n",
    "    'Tokens',\n",
    "    'Chunks',\n",
    "    bins=50\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e00aeb92ffe0dcad",
   "metadata": {},
   "source": [
    "- The chunking strategy is effective, with a good balance between chunk size and overlap.\n",
    "- The maximum chunk length (512) is within the limits of the most embedding models.\n",
    "- The total number of chunks is sufficient for vector indexing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91c004cb6cc357",
   "metadata": {},
   "source": "➡️ Notebook suivant : [02-build_vectordb.ipynb](./02-build_vectordb.ipynb)"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
