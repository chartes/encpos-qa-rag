{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02 - Building the vector database",
   "id": "30600691bf5c8d28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ce notebook a pour objectif d‚Äôindexer les chunks extraits des th√®ses de l‚Äô√âcole nationale des chartes dans des bases vectorielles, en suivant les configurations sp√©cifi√©es dans le fichier `config.yml`.\n",
    "\n",
    "Dans le cadre d'une architecture RAG, cette √©tape correspond √† la mise en place du composant Retriever.\n",
    "\n",
    "> Dans le notebook pr√©c√©dent, nous avons pr√©par√© les chunks selon une strat√©gie adapt√©e √† notre cas d‚Äôusage et stock√© dans un fichier csv.\n",
    "\n",
    "Une fois les chunks encod√©s (embedded), ils sont stock√©s dans une base de donn√©es vectorielle. Lorsqu‚Äôun utilisateur saisit une requ√™te, celle-ci est √† son tour encod√©e par le m√™me mod√®le, puis compar√©e aux vecteurs pr√©sents dans la base pour identifier les documents les plus similaires.\n",
    "\n",
    "Le d√©fi technique principal est donc le suivant :\n",
    "\n",
    "> √âtant donn√© un vecteur de requ√™te, retrouver rapidement ses **k plus proches voisins** dans la base vectorielle, c‚Äôest-√†-dire les k documents les plus pertinents."
   ],
   "id": "7258a0e8f82e5852"
  },
  {
   "cell_type": "code",
   "id": "4d7515da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T17:25:27.025646Z",
     "start_time": "2025-07-08T17:25:20.773989Z"
    }
   },
   "source": [
    "## Imports globaux\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.io_utils import read_yaml\n",
    "from utils.vector_store import VectorDB\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9f2c7612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T17:25:27.041038Z",
     "start_time": "2025-07-08T17:25:27.035446Z"
    }
   },
   "source": [
    "## Chargement de la configuration\n",
    "config = read_yaml(\"../config.yml\")\n",
    "data_path = \"../data/raw/encpos_chunked_tok_512_51.csv\"\n",
    "defaults = config.get(\"defaults\", {})"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "44bceb3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T17:25:29.647430Z",
     "start_time": "2025-07-08T17:25:28.530484Z"
    }
   },
   "source": [
    "## Chargement des donn√©es √† indexer\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Fichier introuvable : {data_path}. Lancez d'abord le notebook de chunking.\")\n",
    "df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "print(f\"Nombre de chunks √† indexer : {len(df)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de chunks √† indexer : 39377\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On prepare les configurations d'indexation vectorielle √† partir du fichier de configuration `config.yml`.",
   "id": "ad2cc6d69892a920"
  },
  {
   "cell_type": "code",
   "id": "1a922b72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T17:25:32.013225Z",
     "start_time": "2025-07-08T17:25:32.006340Z"
    }
   },
   "source": [
    "## G√©n√©ration des configurations d'indexation\n",
    "vector_indexing = []\n",
    "for entry in config.get(\"vector_indexing\", []):\n",
    "    model_id = entry[\"model_id\"]\n",
    "    model = next((m for m in config[\"embedding_models\"] if m[\"id\"] == model_id), None)\n",
    "    if not model:\n",
    "        raise ValueError(f\"Mod√®le non trouv√© : {model_id}\")\n",
    "\n",
    "    for backend in entry[\"backends\"]:\n",
    "        suffix = f\"{model_id}_{backend}\"\n",
    "        name = f\"{model['name']} - {backend.upper()}\"\n",
    "        collection_name = f\"{defaults.get('collection_prefix', 'encpos')}_{model_id}\"\n",
    "        path = os.path.join(defaults.get(\"base_path\", \"data/vectordb\"), suffix)\n",
    "\n",
    "        vector_indexing.append({\n",
    "            \"name\": name,\n",
    "            \"backend\": backend,\n",
    "            \"embedding_model\": model[\"model_path\"],\n",
    "            \"metric\": defaults.get(\"metric\", \"cosine\"),\n",
    "            \"text_column\": defaults.get(\"text_column\", \"full_chunk\"),\n",
    "            \"metadata_columns\": defaults.get(\"metadata_columns\", []),\n",
    "            \"path\": path,\n",
    "            \"qdrant_collection_name\": collection_name,\n",
    "            \"k\": defaults.get(\"k\", 10),\n",
    "            \"force_rebuild\": defaults.get(\"force_rebuild\", False)\n",
    "        })"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Pour indexer efficacement nos donn√©es dans une base vectorielle contenant potentiellement des milliers de documents, il est n√©cessaire de choisir deux √©l√©ments cl√©s :\n",
    "\n",
    "- Une **m√©trique de distance** pour comparer les vecteurs (ex. similarit√© cosinus, distance euclidienne etc.) ;\n",
    "\n",
    "- Un **algorithme de recherche des plus proches voisins** (nearest neighbors search) pour retrouver rapidement les documents pertinents.\n",
    "\n",
    "Dans la cellule suivante nous avons mis en place une boucle principale qui construit une base vectorielle pour chaque configuration d√©finie dans le fichier `config.ym`l.\n",
    "L‚Äôobjectif est de tester diff√©rentes combinaisons de mod√®les d‚Äôembeddings et de backends de stockage.\n",
    "\n",
    "Les deux backends actuellement pris en charge sont :\n",
    "\n",
    "- Faiss : tr√®s rapide pour l‚Äôindexation et la recherche, mais les filtres sur les m√©tadonn√©es sont limit√©s ;\n",
    "\n",
    "- LanceDB : plus lent √† l‚Äôindexation, mais permet des requ√™tes complexes sur les m√©tadonn√©es par exemple en SQL.\n",
    "\n",
    "Nous avons choisi d‚Äôutiliser la similarit√© cosinus comme m√©trique de comparaison entre les vecteurs. Elle mesure l‚Äôangle entre deux vecteurs, ce qui permet de comparer leur direction ind√©pendamment de leur norme. Cela n√©cessite de normaliser tous les vecteurs (c‚Äôest-√†-dire de leur donner une norme unitaire) avant l‚Äôindexation ou la recherche.\n",
    "\n",
    "Pour faciliter l‚Äôindexation, nous avons d√©velopp√© une abstraction Python appel√©e `VectorDB` qui prend en charge :\n",
    "\n",
    "- La normalisation des vecteurs\n",
    "\n",
    "- La cr√©ation des bases vectorielles et leur persistance\n",
    "\n",
    "- L‚Äôindexation des embeddings\n",
    "\n",
    "- La recherche\n",
    "\n",
    "Cette abstraction nous permet de comparer diff√©rents mod√®les et backends de mani√®re uniforme, et de les √©valuer dans des conditions √©quitables.\n",
    "\n"
   ],
   "id": "1b548879d9b99db8"
  },
  {
   "cell_type": "code",
   "id": "85445417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:17:40.710798Z",
     "start_time": "2025-07-08T17:25:38.262535Z"
    }
   },
   "source": [
    "%%time\n",
    "## Boucle principale d‚Äôindexation\n",
    "# 2h55 pour indexer les chunks dans 6 vectorstore\n",
    "# UMAP :\n",
    "UMAP_PROJECTION = False\n",
    "#df = df.sample(n=50)\n",
    "\n",
    "def get_stored_vectors(db: VectorDB) -> np.ndarray:\n",
    "    if db.backend == \"faiss\":\n",
    "        # db.index.index est un IndexFlat ou autre structure FAISS\n",
    "        return np.array(db.index.index.reconstruct_n(0, db.index.index.ntotal))\n",
    "\n",
    "    elif db.backend == \"lancedb\":\n",
    "        df = db.index.to_pandas()\n",
    "        return np.vstack(df[\"vector\"].values)  # Chaque vecteur est une liste ‚Üí stack en matrice\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Backend {db.backend} non support√©\")\n",
    "\n",
    "for conf in vector_indexing:\n",
    "    print(\"\\n--- Indexation en cours ---\")\n",
    "    print(\"Nom:\", conf[\"name\"])\n",
    "\n",
    "    db = VectorDB(\n",
    "        backend=conf[\"backend\"],\n",
    "        embedding_model=conf[\"embedding_model\"],\n",
    "        metric=conf[\"metric\"],\n",
    "        path=conf[\"path\"],\n",
    "        k=conf[\"k\"],\n",
    "        force_rebuild=conf[\"force_rebuild\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    db.add_from_dataframe(\n",
    "        df=df,\n",
    "        text_column=conf[\"text_column\"],\n",
    "        metadata_columns=conf[\"metadata_columns\"]\n",
    "    )\n",
    "\n",
    "    db.save()\n",
    "    print(\"üì¶ Indexation termin√©e pour:\", conf[\"name\"])\n",
    "\n",
    "    if UMAP_PROJECTION:\n",
    "        # G√©n√©ration de la projection UMAP\n",
    "        print(\"‚Üí G√©n√©ration de la projection UMAP pour:\", conf[\"name\"])\n",
    "        umap_path = os.path.join(\"scripts\", conf[\"path\"], \"projection_umap.npy\")\n",
    "        labels_path = os.path.join(\"scripts\", conf[\"path\"], \"projection_labels.pkl\")\n",
    "        if os.path.isfile(umap_path) and os.path.isfile(labels_path):\n",
    "            print(\"üîÅ Projection UMAP d√©j√† g√©n√©r√©e, chargement depuis le disque.\")\n",
    "        else:\n",
    "            print(\"üõ†Ô∏è Calcul de la projection UMAP...\")\n",
    "            vectors = get_stored_vectors(db)\n",
    "            reducer = umap.UMAP(n_components=2, random_state=42, n_jobs=-1)\n",
    "            reduced = reducer.fit_transform(np.array(vectors))\n",
    "            joblib.dump(reducer, os.path.join(conf[\"path\"], \"umap_model.joblib\"))\n",
    "            np.save(umap_path, reduced)\n",
    "            with open(labels_path, \"wb\") as f:\n",
    "                pickle.dump(\n",
    "                    df[[\"author\", \"position_name\", \"year\"]].to_dict(\"records\"), f\n",
    "                )\n",
    "            print(\"‚úÖ Projection UMAP sauvegard√©e dans:\", umap_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Indexation en cours ---\n",
      "Nom: CamemBERT Large - LANCEDB\n",
      "üì¶ Initialisation de LanceDB √† data/vectordb/camembert-large_lancedb\n",
      "üÜï Table LanceDB 'camembert-large_lancedb' √† cr√©er lors de l'indexation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pr√©paration des documents pour lancedb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39377/39377 [00:01<00:00, 21462.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Cr√©ation de la table LanceDB 'camembert-large_lancedb' √† partir des documents...\n",
      "üì¶ Indexation termin√©e pour: CamemBERT Large - LANCEDB\n",
      "\n",
      "--- Indexation en cours ---\n",
      "Nom: CamemBERT Base - LANCEDB\n",
      "üì¶ Initialisation de LanceDB √† data/vectordb/camembert-base_lancedb\n",
      "üÜï Table LanceDB 'camembert-base_lancedb' √† cr√©er lors de l'indexation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pr√©paration des documents pour lancedb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39377/39377 [00:01<00:00, 37788.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Cr√©ation de la table LanceDB 'camembert-base_lancedb' √† partir des documents...\n",
      "üì¶ Indexation termin√©e pour: CamemBERT Base - LANCEDB\n",
      "\n",
      "--- Indexation en cours ---\n",
      "Nom: Multilingual DistilUSE - LANCEDB\n",
      "üì¶ Initialisation de LanceDB √† data/vectordb/multilingual_lancedb\n",
      "üÜï Table LanceDB 'multilingual_lancedb' √† cr√©er lors de l'indexation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pr√©paration des documents pour lancedb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39377/39377 [00:01<00:00, 36957.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Cr√©ation de la table LanceDB 'multilingual_lancedb' √† partir des documents...\n",
      "üì¶ Indexation termin√©e pour: Multilingual DistilUSE - LANCEDB\n",
      "CPU times: user 4min 24s, sys: 3min 8s, total: 7min 32s\n",
      "Wall time: 52min 2s\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualisation des r√©sultats\n",
    "\n",
    "### Carte UMAP des embeddings\n",
    "\n",
    "Durant la boucle d‚Äôindexation, nous avons √©galement g√©n√©r√© un mod√®le de projection des embeddings pour chaque configuration. Cette projection permet de visualiser la distribution des chunks dans un espace 2D via un algorithme de r√©duction de dimensionnalit√© des vecteurs d'embeddings.\n",
    "\n",
    "Nous utilisons ici UMAP (Uniform Manifold Approximation and Projection) mais il en existe d'autres comme PCA, t-SNE, PaCMAP etc.\n",
    "\n",
    "Sur le graphique ci-dessous, on observe une repr√©sentation spatiale des documents de la base de connaissances.\n",
    "Chaque point correspond √† un document, repr√©sent√© par son embedding vectoriel, qui encode son contenu s√©mantique.\n",
    "\n",
    "L'intution ici est que plus deux documents sont proches en termes de sens, plus leurs vecteurs devraient l‚Äô√™tre √©galement dans l‚Äôespace vectoriel.\n",
    "\n",
    "L‚Äôembedding de la requ√™te utilisateur est √©galement repr√©sent√© sur le graphique.\n",
    "\n",
    "L‚Äôobjectif ici est de retrouver les k documents dont le sens est le plus proche de cette requ√™te.\n",
    "\n",
    "Les projections nous aident √† visualiser la distribution des documents dans l‚Äôespace vectoriel et √† identifier les √©ventuels clusters ou regroupements de chun similaires"
   ],
   "id": "dee931056f69ba19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Exemple d‚Äôutilisation et visualisation\n",
    "example_conf = vector_indexing[0]\n",
    "print(\n",
    "    f\"\\nüîç Configuration d'exemple : {example_conf['name']}\\n\"\n",
    ")"
   ],
   "id": "e7c4618cff442423",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8a85b1f",
   "metadata": {},
   "source": [
    "if example_conf:\n",
    "        print(f\"\\nüéØ Exemple : utilisation du retriever {example_conf['name']}\")\n",
    "\n",
    "        db = VectorDB(\n",
    "            backend=example_conf[\"backend\"],\n",
    "            embedding_model=example_conf[\"embedding_model\"],\n",
    "            metric=example_conf[\"metric\"],\n",
    "            path=example_conf[\"path\"],\n",
    "            k=5,\n",
    "            force_rebuild=False\n",
    "        )\n",
    "\n",
    "        query = \"Quel est le temps de L√©on Marchand au 400m papillon aux Jeux Olympiques de Paris de 2025 ?\"\n",
    "        results = db.query(query)\n",
    "\n",
    "        print(\"\\nüß† R√©sultats de la requ√™te :\")\n",
    "        for i, r in enumerate(results[:5]):\n",
    "            print(i, r)\n",
    "\n",
    "        print(f\"\\nüé® Affichage de la projection UMAP :{example_conf['name']}\")\n",
    "        umap_path = os.path.join(example_conf[\"path\"], \"projection_umap.npy\")\n",
    "        labels_path = os.path.join(example_conf[\"path\"], \"projection_labels.pkl\")\n",
    "        # load previously fitted UMAP reducer\n",
    "        reducer = joblib.load(os.path.join(example_conf[\"path\"], \"umap_model.joblib\"))\n",
    "        if os.path.exists(umap_path) and os.path.exists(labels_path):\n",
    "            emb_2d = np.load(umap_path)\n",
    "            with open(labels_path, \"rb\") as f:\n",
    "                labels = pickle.load(f)\n",
    "\n",
    "            # Requ√™te utilisateur (embedding + projection)\n",
    "            user_embedding = db.embed_query(query)\n",
    "\n",
    "\n",
    "            query_proj = reducer.transform(reducer.transform(np.array(user_embedding).reshape(1, -1)))\n",
    "\n",
    "            # Cr√©ation du DataFrame pour Plotly\n",
    "            df_plot = pd.DataFrame.from_dict(\n",
    "                [\n",
    "                    {\n",
    "                        \"x\": emb_2d[i, 0],\n",
    "                        \"y\": emb_2d[i, 1],\n",
    "                        \"source\": f\"{labels[i].get('author', 'Inconnu')}\",\n",
    "                        \"extract\": f\"{labels[i].get('position_name', '')} : {labels[i].get('text', '')[:100]}...\",\n",
    "                        \"symbol\": \"circle\",\n",
    "                        \"size_col\": 4,\n",
    "                    }\n",
    "                    for i in range(len(labels))\n",
    "                ]\n",
    "                + [\n",
    "                    {\n",
    "                        \"x\": query_proj[0, 0],\n",
    "                        \"y\": query_proj[0, 1],\n",
    "                        \"source\": \"User query\",\n",
    "                        \"extract\": query,\n",
    "                        \"symbol\": \"star\",\n",
    "                        \"size_col\": 100,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Visualisation avec Plotly\n",
    "            fig = px.scatter(\n",
    "                df_plot,\n",
    "                x=\"x\",\n",
    "                y=\"y\",\n",
    "                color=\"source\",\n",
    "                hover_data=[\"extract\"],\n",
    "                size=\"size_col\",\n",
    "                symbol=\"symbol\",\n",
    "                color_discrete_map={\"User query\": \"black\"},\n",
    "                width=1000,\n",
    "                height=700,\n",
    "            )\n",
    "            fig.update_traces(\n",
    "                marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
    "                selector=dict(mode=\"markers\"),\n",
    "            )\n",
    "            fig.update_layout(\n",
    "                legend_title_text=\"<b>Chunk source</b>\",\n",
    "                title=f\"<b>2D Projection of Positions de th√®ses embeddings via UMAP ({example_conf['embedding_model']})</b>\",\n",
    "                plot_bgcolor=\"white\",\n",
    "            )\n",
    "            fig.show()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Fichiers de projection manquants. Relancer l‚Äô√©tape UMAP si besoin.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "‚û°Ô∏è Notebook suivant : [03-assemble_rag.ipynb](./03-assemble_rag.ipynb)",
   "id": "6812a98481fd9b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef67d4eced927718"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
